{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14799051,"sourceType":"datasetVersion","datasetId":9462413}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q bitsandbytes transformers datasets peft accelerate\n\nimport os\nos.environ[\"WANDB_DISABLED\"] = \"true\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-07T19:51:46.197213Z","iopub.execute_input":"2026-02-07T19:51:46.197511Z","iopub.status.idle":"2026-02-07T19:51:57.381449Z","shell.execute_reply.started":"2026-02-07T19:51:46.197486Z","shell.execute_reply":"2026-02-07T19:51:57.380731Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m530.9/530.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hCollecting bitsandbytes\n  Downloading bitsandbytes-0.49.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\nRequirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.8.0+cu126)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (26.0rc2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.20.3)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (75.2.0)\nRequirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2025.10.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.80)\nRequirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\nRequirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.4.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.3.0.4)\nRequirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.7.77)\nRequirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.7.1.2)\nRequirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.5.4.2)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\nRequirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.27.3)\nRequirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.85)\nRequirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.11.1.6)\nRequirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.4.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\nDownloading bitsandbytes-0.49.1-py3-none-manylinux_2_24_x86_64.whl (59.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.49.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nsecret_label = \"hf_token\"\nhf_token = UserSecretsClient().get_secret(secret_label)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T19:51:57.382879Z","iopub.execute_input":"2026-02-07T19:51:57.383149Z","iopub.status.idle":"2026-02-07T19:51:57.464262Z","shell.execute_reply.started":"2026-02-07T19:51:57.383102Z","shell.execute_reply":"2026-02-07T19:51:57.463558Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import json\nimport torch\nimport numpy as np\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    TrainingArguments,\n    Trainer,\n    DataCollatorForLanguageModeling,\n    BitsAndBytesConfig\n)\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nfrom datasets import Dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T18:29:04.841118Z","iopub.execute_input":"2026-02-07T18:29:04.841368Z","iopub.status.idle":"2026-02-07T18:29:33.516700Z","shell.execute_reply.started":"2026-02-07T18:29:04.841341Z","shell.execute_reply":"2026-02-07T18:29:33.516155Z"}},"outputs":[{"name":"stderr","text":"2026-02-07 18:29:17.750276: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1770488957.926456      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1770488957.975513      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1770488958.380340      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1770488958.380371      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1770488958.380373      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1770488958.380376      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"#### - Note: Kaggle is fickle at times. The dataset created on Kaggle sometimes appear under /kaggle/input/ or under /kaggle/input/datasets/username/; the path names could use either `_` or `-`. So list the files to get the correct path if you encounter a `File Not Found Error`\n\nSteps to run\n- Create datasets on kaggle with the training-data/finetuning and training-data/splits from notebook 04-prepare-training-data\n- Change the `DIMENSION` parameter one at a time. The list of values are [\"accuracy\", \"clarity\", \"completeness\", compliance\", \"risk\"]\n- Click on Save Version at the top right corner\n- Leave default version name, and ensure Version Type is Save & Run All (Commit). Click Continue\n- Check log for successful completion\n- Access the files from the Output tab. Download them, and then upload them to Kaggle as a dataset","metadata":{}},{"cell_type":"markdown","source":"### ============================================================================\n### CONFIGURATION\n### ============================================================================","metadata":{}},{"cell_type":"code","source":"DIMENSION = \"risk\"\nMODEL_NAME = \"google/medgemma-4b-it\"\nMAX_SEQ_LENGTH = 2048\nLORA_RANK = 16\nOUTPUT_DIR = f\"/kaggle/working/models/{DIMENSION}_agent\"\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T18:42:13.244353Z","iopub.execute_input":"2026-02-07T18:42:13.244989Z","iopub.status.idle":"2026-02-07T18:42:13.248408Z","shell.execute_reply.started":"2026-02-07T18:42:13.244961Z","shell.execute_reply":"2026-02-07T18:42:13.247774Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"### ============================================================================\n### LOAD DATA\n### ============================================================================","metadata":{}},{"cell_type":"code","source":"def load_json(filepath):\n    with open(filepath, 'r') as f:\n        return json.load(f)\n\nprint(f\"Loading {DIMENSION} training data...\")\ntrain_data = load_json(f\"/kaggle/input/datasets/laxmsun/training-data/finetuning/{DIMENSION}_train.json\")\ntest_data = load_json(f\"/kaggle/input/datasets/laxmsun/training-data/finetuning/{DIMENSION}_test.json\")\n\nprint(f\"Train: {len(train_data)}, Test: {len(test_data)}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T18:42:15.000260Z","iopub.execute_input":"2026-02-07T18:42:15.000811Z","iopub.status.idle":"2026-02-07T18:42:15.026453Z","shell.execute_reply.started":"2026-02-07T18:42:15.000783Z","shell.execute_reply":"2026-02-07T18:42:15.025906Z"}},"outputs":[{"name":"stdout","text":"Loading completeness training data...\nTrain: 120, Test: 30\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"### ============================================================================\n### FORMAT DATA FOR TRAINING\n### ============================================================================","metadata":{}},{"cell_type":"code","source":"\n# Convert to HuggingFace Dataset\ntrain_dataset = Dataset.from_list(train_data)\n\ntest_dataset = Dataset.from_list(test_data)\n\nprint(\"\\nSample training example:\")\nprint(train_dataset[0]['text'][:500])\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### ============================================================================\n### LOAD MODEL WITH LORA\n### ============================================================================","metadata":{}},{"cell_type":"code","source":"\n\nprint(\"\\nLoading model...\")\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16\n)\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    trust_remote_code=True,\n    token = hf_token,\n)\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, token=hf_token)\ntokenizer.pad_token = tokenizer.eos_token\n\n# Prepare for LoRA\nmodel = prepare_model_for_kbit_training(model)\n\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=16,\n    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n\nmodel = get_peft_model(model, lora_config)\nprint(f\"Trainable params: {model.print_trainable_parameters()}\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### ============================================================================\n### TRAINING CONFIGURATION\n### ============================================================================","metadata":{}},{"cell_type":"code","source":"def tokenize_function(examples):\n    return tokenizer(\n        examples['text'],\n        truncation=True,\n        max_length=2048,\n        padding=\"max_length\"\n    )\n\ntokenized_dataset = train_dataset.map(\n    tokenize_function,\n    batched=True,\n    remove_columns=[\"text\", \"note_id\"]\n)\n\ndef tokenize_function(examples):\n    return tokenizer(examples['text'], truncation=True, max_length=2048)\n\ntokenized_dataset = train_dataset.map(\n    tokenize_function,\n    batched=True,\n    remove_columns=[\"text\", \"note_id\"]\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### ============================================================================\n### TRAINING\n### ============================================================================","metadata":{}},{"cell_type":"code","source":"\ntraining_args = TrainingArguments(\n    output_dir=f\"./{DIMENSION}_agent\",\n    num_train_epochs=3,\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=8,\n    learning_rate=2e-4,\n    logging_steps=5,\n    save_strategy=\"epoch\",\n    fp16=True,\n)\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### ============================================================================\n### FINE-TUNE\n### ============================================================================","metadata":{}},{"cell_type":"code","source":"\n\nprint(\"\\nStarting fine-tuning...\")\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset,\n    data_collator=data_collator,\n)\n\nprint(\"\\nTraining...\")\ntrainer.train()\n\n\nprint(\"\\nTraining complete!\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### ============================================================================\n### SAVE MODEL\n### ============================================================================","metadata":{}},{"cell_type":"code","source":"\n\nprint(f\"\\nSaving model to {OUTPUT_DIR}...\")\nmodel.save_pretrained(OUTPUT_DIR)\ntokenizer.save_pretrained(OUTPUT_DIR)\n\n# # Also save merged model (LoRA + base)\n# print(\"Saving merged model...\")\n# model.save_pretrained_merged(\n#     f\"{OUTPUT_DIR}/merged\",\n#     tokenizer,\n#     save_method=\"merged_16bit\",\n# )\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### ============================================================================\n### EVALUATION\n### ============================================================================","metadata":{}},{"cell_type":"code","source":"print(\"\\n\" + \"=\"*80)\nprint(\"EVALUATION\")\nprint(\"=\"*80)\n\nmodel.eval()\n\ndef predict_score(note_text):\n    \"\"\"Predict score for a clinical note\"\"\"\n    # Create prompt in Gemma format\n    prompt = (\n        f\"<start_of_turn>user\\n\"\n        f\"Score the {DIMENSION} of this clinical note (0-100):\\n\\n\"\n        f\"{note_text[:500]}\\n\"  # Truncate long notes\n        f\"<end_of_turn>\\n\"\n        f\"<start_of_turn>model\\n\"\n    )\n    \n    inputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True).to(\"cuda\")\n    \n    with torch.no_grad():\n        output = model.generate(\n            **inputs,\n            max_new_tokens=10,\n            do_sample=True,\n            temperature=0.4,\n            pad_token_id=tokenizer.eos_token_id\n        )\n    \n    # Extract only generated tokens\n    prompt_length = inputs.input_ids.shape[1]\n    generated_tokens = output[0][prompt_length:]\n    response = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()\n    \n    # Parse number from response\n    try:\n        return int(response)\n    except:\n        import re\n        numbers = re.findall(r'\\d+', response)\n        return int(numbers[0]) if numbers else 50  # Default fallback\n\n# Load test data\ntest_data = load_json(f\"/kaggle/input/datasets/laxmsun/training-data/finetuning/{DIMENSION}_test.json\")\ntest_split = load_json(\"/kaggle/input/datasets/laxmsun/training-data/splits/test_split.json\")\n\npredictions = []\nactuals = []\n\nprint(f\"\\nEvaluating on {min(20, len(test_data))} test samples...\")\n\nfor i, item in enumerate(test_data[:20]):\n    # Find corresponding note\n    note = next((n for n in test_split if n['note_id'] == item['note_id']), None)\n    \n    if note is None:\n        print(f\"Warning: Note {item['note_id']} not found in test split\")\n        continue\n    \n    pred = predict_score(note['note_text'])\n    output_part = item['text'].split(\"<start_of_turn>model\\n\")[1].split(\"<end_of_turn>\")[0]\n    actual = int(output_part)\n    \n    predictions.append(pred)\n    actuals.append(actual)\n    \n    if i < 5:  # Show first 5\n        print(f\"{item['note_id']}: Predicted={pred}, Actual={actual}, Error={abs(pred-actual)}\")\n\n# Calculate metrics\npredictions = np.array(predictions)\nactuals = np.array(actuals)\n\nmae = np.mean(np.abs(predictions - actuals))\nrmse = np.sqrt(np.mean((predictions - actuals)**2))\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"RESULTS\")\nprint(\"=\"*80)\nprint(f\"MAE: {mae:.2f}\")\nprint(f\"RMSE: {rmse:.2f}\")\nprint(f\"Within 10pts: {np.mean(np.abs(predictions - actuals) <= 10)*100:.1f}%\")\nprint(f\"Within 20pts: {np.mean(np.abs(predictions - actuals) <= 20)*100:.1f}%\")\n\n\nresults = {\n    \"dimension\": DIMENSION,\n    \"mae\": float(mae),\n    \"rmse\": float(rmse),\n    \"within_10\": float(np.mean(np.abs(predictions - actuals) <= 10)*100),\n    \"within_20\": float(np.mean(np.abs(predictions - actuals) <= 20)*100),\n    \"predictions\": predictions.tolist(),\n    \"actuals\": actuals.tolist(),\n}\n\nwith open(f\"{OUTPUT_DIR}/results.json\", 'w') as f:\n    json.dump(results, f, indent=2)\n\nprint(f\"\\nResults saved to {OUTPUT_DIR}/results.json\")\nprint(\"Done!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}